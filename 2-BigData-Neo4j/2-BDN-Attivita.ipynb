{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAx8hs2MSUsv"
   },
   "source": [
    "# Recommendation usecase – neo4j sandbox e Python\n",
    "Simone Richetti, mat. 129180, attività 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXF3vYaM_RYg"
   },
   "source": [
    "## Traccia\n",
    "\n",
    "Completare lo studio della [Recommendation Sandbox](http://neo4j.com/sandbox-v2/) vista a lezione, provando e studiando le tecniche avanzate. Quindi:\n",
    "*   **Collaborative filtering, componente temporale**: come si potrebbe sfruttare il timestamp dei rating per generare suggerimenti più interessanti? Commentare e costruire (almeno) due interrogazioni per mostrarlo;\n",
    "*   **Content-based filtering, keyword**: come si potrebbero migliorare i tratti comuni considerati sfruttando le parole chiave disponibili nella trama (plot) dei film? Commentare come si potrebbero modellare tali parole chiave nel grafo, quindi scrivere un programma Python che estenda il grafo estraendo e  memorizzando in modo opportuno (una parte di) tali parole (semplificazione opzionale senza Python: estendere una porzione di grafo manualmente via comandi cypher). Infine, scrivere (almeno) due esempi di interrogazione che le sfrutti, estendendo un esempio della sandbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPD7dExIVnr5"
   },
   "source": [
    "## Dipendenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueLyxe11VpHq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\riche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\riche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install neo4j-driver\n",
    "from neo4j.v1 import GraphDatabase, basic_auth\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gc0KbAUrTQ1L"
   },
   "source": [
    "## Dati utilizzati e setup Neo4j Sandbox\n",
    "Per questo progetto è stato utilizzato il dataset contenuto nella sandbox **Recommendation** di Neo4j. È possibile effettuare il setup dell'ambiente in pochi semplici passi:\n",
    "1.   Effettuare il login e/o registrarsi al link delle [Sandbox Neo4j](https://sandbox.neo4j.com/);\n",
    "2.   Creare un nuovo progetto e selezionare il progetto *Recommendation*;\n",
    "3.   Nella sezione *Connect via drivers* selezionare il radiobutton *Python*: recuperare nello snippet di codice l'URL e le credenziali con cui connettersi alla sandbox e inserirle nella *code cell* sottostante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khLGhTZKXs4M"
   },
   "outputs": [],
   "source": [
    "# Overwrite values with yours\n",
    "SANDBOX_URL = \"bolt://XXX.XX.XXX.XXX:XXXXX\"\n",
    "SANDBOX_USERNAME = \"neo4j\"\n",
    "SANDBOX_PASSWD = \"insert_password_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-zFz7QMYujn"
   },
   "source": [
    "Ora è possibile connettersi via script Python alla sandbox per effettuare *cypher queries* su di essa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkQc4FkIWs6V"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot resolve address XXX.XX.XXX.XXX:XXXXX",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\addressing.py\u001b[0m in \u001b[0;36m_dns_resolve\u001b[1;34m(cls, address, family)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 10109] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dd00c1c6933f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m driver = GraphDatabase.driver(\n\u001b[0;32m      2\u001b[0m     \u001b[0mSANDBOX_URL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     auth=basic_auth(SANDBOX_USERNAME, SANDBOX_PASSWD))\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\__init__.py\u001b[0m in \u001b[0;36mdriver\u001b[1;34m(cls, uri, auth, **config)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mclosed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \"\"\" Return :const:`True` if closed, :const:`False` otherwise.\n\u001b[0;32m    183\u001b[0m         \"\"\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\__init__.py\u001b[0m in \u001b[0;36mbolt_driver\u001b[1;34m(cls, target, auth, **config)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mConnections\u001b[0m \u001b[0mestablished\u001b[0m \u001b[0mby\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDirectDriver\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mare\u001b[0m \u001b[0malways\u001b[0m \u001b[0mmade\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mexact\u001b[0m \u001b[0mhost\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mport\u001b[0m \u001b[0mdetailed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mURI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \"\"\"\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0muri_scheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"bolt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, target, auth, **config)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, address, auth, pool_config, workspace_config)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36macquire\u001b[1;34m(self, access_mode, timeout, database)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36m_acquire\u001b[1;34m(self, address, timeout)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36mopener\u001b[1;34m(addr, timeout)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, address, auth, timeout, **pool_config)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\io\\__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(address, timeout, custom_resolver, ssl_context, keep_alive)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\addressing.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self, family, resolver)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\big-data-projects\\lib\\site-packages\\neo4j\\addressing.py\u001b[0m in \u001b[0;36m_dns_resolve\u001b[1;34m(cls, address, family)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot resolve address XXX.XX.XXX.XXX:XXXXX"
     ]
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "    SANDBOX_URL, \n",
    "    auth=basic_auth(SANDBOX_USERNAME, SANDBOX_PASSWD))\n",
    "session = driver.session()\n",
    "\n",
    "# Query and print node labels\n",
    "cypher_query = '''\n",
    "CALL db.labels()\n",
    "'''\n",
    "\n",
    "results = session.run(cypher_query,\n",
    "  parameters={})\n",
    "\n",
    "print('NODE LABELS:')\n",
    "for record in results:\n",
    "  print('*', record['label'])\n",
    "print('')\n",
    "\n",
    "# Query and print relationships\n",
    "cypher_query = '''\n",
    "CALL db.relationshipTypes()\n",
    "'''\n",
    "\n",
    "results = session.run(cypher_query,\n",
    "  parameters={})\n",
    "\n",
    "print('RELATIONSHIP TYPES:')\n",
    "for record in results:\n",
    "  print('*', record['relationshipType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEytz7VEhS34"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTiIiYexVTxz"
   },
   "source": [
    "# Parte 1 - Collaborative Filtering: componente temporale\n",
    "Gli algoritmi di tipo *collaborative filtering* sfruttano le informazioni legate alle recensioni degli utenti per fornire *recommendations* rilevanti. La sandbox fornisce diversi esempi più e meno sofisticati di queries che sfruttano questo tipo di informazioni. In questa parte verranno proposte tre queries che cercano di utilizzare, in aggiunta alle altre informazioni, anche il timestamp delle recensioni per trovare suggerimenti significativi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fm4CZihaPc6b"
   },
   "source": [
    "## Query 1.1 - Tendenze\n",
    "Un primo esempio molto semplice è quello dell'individuazione dei film \"in tendenza\", ovvero i film più visti nell'ultimo periodo.\n",
    "\n",
    "Questo è un esempio atipico di *recommendation* poichè non è mirato direttamente ad un utente ma è identico per tutti gli utenti, nonostante ciò rimane un sistema di suggerimenti molto utilizzato su numerose piattaforme e basato esclusivamente su rating e timestamp di tutte le reviews del sistema.\n",
    "\n",
    "Poichè il dataset non è costantemente aggiornato, si considerano le tendenze relative agli ultimi due mesi in cui sono presenti recensioni. Considerando solo le recensioni create in quel periodo, si riportano i dieci film che hanno ricevuto le migliori recensioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6EjhTDOPfAh"
   },
   "outputs": [],
   "source": [
    "last_review_time_query = '''MATCH ()-[r:RATED]->()\n",
    "RETURN r.timestamp AS time\n",
    "ORDER BY time DESC\n",
    "LIMIT 1\n",
    "'''\n",
    "trending_query = '''MATCH (:User)-[r:RATED]->(m:Movie)\n",
    "WHERE $referenceTime - r.timestamp < 5256000\n",
    "RETURN m.title, avg(r.rating) AS ratings\n",
    "ORDER BY ratings DESC\n",
    "LIMIT 30\n",
    "'''\n",
    "last_timestamp = session.run(last_review_time_query).single()['time']\n",
    "results = session.run(trending_query, parameters={'referenceTime': last_timestamp})\n",
    "print(\"TRENDING MOVIES:\")\n",
    "for result in results:\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqIHAT1HAflg"
   },
   "source": [
    "## Query 1.2 - Suggerimenti per recensioni simili\n",
    "Questa query si differenzia dalla precedente in quanto diretta ad uno specifico utente e per una maggior complessità. Di seguito viene spiegato il ragionamento applicato e il codice in *cypher language* della query:\n",
    "1. **Si identificano le recensioni più importanti e più recenti per l'utente.** Si selezionano solo le recensioni che hanno una media di 4 o più, si ordinano per *timestamp* decrescente e si prendono solo le prime 3. Queste sono le recensioni che rappresentano le preferenze dell'utente in questo momento:\n",
    "\n",
    "        MATCH (m1:Movie)<-[r1:RATED]-(ref_u:User {name: $targetUserName})\n",
    "        WHERE r1.rating >= 4.0\n",
    "        WITH m1, r1, ref_u \n",
    "        ORDER BY r1.timestamp DESC \n",
    "        LIMIT 3\n",
    "\n",
    "2. **Si identificano le recensioni più simili ad esse, trovando così gli utenti con gusti simili nello stesso arco temporale**. Si cercano le recensioni di altri utenti allo stesso film, con voti alti e timestamp simile (differenza tra i due timestamp inferiore ai tre mesi):\n",
    "\n",
    "        MATCH (m1:Movie)<-[r2:RATED]-(sim_u:User)\n",
    "        WHERE r2.rating >= 4.0\n",
    "        AND abs(r1.timestamp - r2.timestamp) < 7884000\n",
    "        AND sim_u.userId<>ref_u.userId\n",
    "        WITH sim_u, ref_u, m1, r1\n",
    "\n",
    "3. **Si identificano i film meglio recensiti dagli utenti con gusti comuni**. Si applica la stessa ricerca del punto 1 agli utenti trovati nel punto 2, così da trovare i film piaciuti nel periodo recente agli utenti con gusti simili: questi film costituiscono i nostri suggerimenti all'utente di riferimento.\n",
    "\n",
    "        MATCH (sugg_m:Movie)<-[r3:RATED]-(sim_u:User)\n",
    "        WHERE r3.rating >= 4.0\n",
    "        AND abs(r1.timestamp - r3.timestamp) < 7884000\n",
    "        AND NOT (sugg_m)<-[:RATED]-(ref_u)\n",
    "        RETURN DISTINCT sugg_m.title AS title\n",
    "\n",
    "È possibile modificare l'utente su cui applicare la query modificando la variabile `TARGET_USER_NAME`: si noti però che questa query non garantisce di trovare soluzioni poichè per alcuni utenti i vincoli sul timestamp delle recensioni potrebbero essere molto stringenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhtHuTIJsdDJ"
   },
   "outputs": [],
   "source": [
    "TARGET_USER_NAME = 'Cynthia Freeman'\n",
    "\n",
    "best_reviews_cypher = '''\n",
    "MATCH (m1:Movie)<-[r1:RATED]-(ref_u:User {name: $targetUserName})\n",
    "WHERE r1.rating >= 4.0\n",
    "WITH m1, r1, ref_u \n",
    "ORDER BY r1.timestamp DESC \n",
    "LIMIT 3\n",
    "\n",
    "MATCH (m1:Movie)<-[r2:RATED]-(sim_u:User)\n",
    "WHERE r2.rating >= 4.0\n",
    "AND abs(r1.timestamp - r2.timestamp) < 7884000\n",
    "AND sim_u.userId<>ref_u.userId\n",
    "WITH sim_u, ref_u, m1, r1\n",
    "\n",
    "MATCH (sugg_m:Movie)<-[r3:RATED]-(sim_u:User)\n",
    "WHERE r3.rating >= 4.0\n",
    "AND abs(r1.timestamp - r3.timestamp) < 7884000\n",
    "AND NOT (sugg_m)<-[:RATED]-(ref_u)\n",
    "RETURN DISTINCT sugg_m.title AS title\n",
    "LIMIT 30\n",
    "'''\n",
    "\n",
    "results = session.run(best_reviews_cypher, parameters={'targetUserName': TARGET_USER_NAME})\n",
    "print(\"RECOMMENDED MOVIES FOR\", TARGET_USER_NAME, \":\")\n",
    "for result in results:\n",
    "  print('*', result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9oAa40OR--Y"
   },
   "source": [
    "## Query 1.3 - Estensione esempio Cosine Similarity\n",
    "Questa query si propone di estendere una delle query presentate tra gli esempi della sandbox. In questo esempio viene utilizzata la misura detta *Cosine Similarity* per calcolare la similarità tra due utenti.\n",
    "\n",
    "La **Cosine Similarity** tra due insiemi $A$ e $B$ è calcolata come:\n",
    "\n",
    "$$sim(A,B) = \\frac{A \\cdot B}{\\|A\\| \\times \\|B\\|} = \\frac{\\sum_{i=1}^n A_i \\times B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\times \\sqrt{\\sum_{i=1}^n B_i^2}},\\\\\n",
    "sim(A,B) \\in [-1,1].$$\n",
    "\n",
    "Nell'esempio della sandbox, $A$ e $B$ sono vettori contenenti i voti dati da due utenti ai film che hanno visto entrambi. Più questi due vettori sono simili, più la similarità è alta.\n",
    "\n",
    "Nella nostra query, calcoleremo non solo una similarità in termini di rating simili allo stesso film ma anche una similarità in termini di tempistiche, ovvero timestamp simili delle recensioni agli stessi film. Questa similarità andrà combinata con la precedente per dare più peso alle recensioni che hanno un rating simile pubblicate nello stesso periodo. In questo modo potremo individuare in maniera più accurata gli utenti più simili tra loro.\n",
    "\n",
    "Andiamo quindi nel dettaglio del codice *cypher* della query per spiegarne il funzionamento:\n",
    "\n",
    "1. Partendo dall'utente target, consideriamo tutti gli utenti che hanno recensito gli stessi film recensiti da lui. Per ciascun utente creiamo due vettori, uno contenente i rating delle sue recensioni e uno contenente i timestamp:\n",
    "\n",
    "        MATCH (p1:User {name: $targetUserName})-[x:RATED]->(movie)<-[x2:RATED]-(p2:User)\n",
    "        WHERE p2 <> p1\n",
    "        WITH p1, p2, \n",
    "            collect(x.rating) AS p1Ratings, collect(x2.rating) AS p2Ratings,\n",
    "            collect(x.timestamp) AS p1Times, collect(x2.timestamp) AS p2Times\n",
    "\n",
    "2. Considerando solo gli utenti che hanno più di 10 film in comune con l'utente target, calcoliamo la *Cosine Similarity* per ratings e per timestamp:\n",
    "\n",
    "        WHERE size(p1Ratings) > 10\n",
    "        WITH p1, p2,\n",
    "            algo.similarity.cosine(p1Ratings, p2Ratings) AS ratingSimilarity,\n",
    "            algo.similarity.cosine(p1Times, p2Times) AS timeSimilarity\n",
    "\n",
    "3. Calcoliamo quindi la similarity complessiva tra i due utenti come media pesata delle due similarità precedentemente calcolate, in cui i ratings pesano 3 volte tanto i timestamp. Consideriamo infine i 10 utenti con similarità più alta:\n",
    "\n",
    "        WITH p1, p2, ((3*ratingSimilarity)+timeSimilarity)/4 AS similarity\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT 10\n",
    "\n",
    "4. Una volta identificati gli utenti più simili, i film consigliati saranno i film che questi ultimi hanno più gradito. Sono quindi considerati i film recensiti da questo sottoinsieme degli utenti e si selezionano i dieci film con media voti migliore:\n",
    "        MATCH (p2)-[x3:RATED]->(rec:Movie)\n",
    "        WHERE NOT EXISTS ( (p1)-[:RATED]->(rec) )\n",
    "        RETURN rec.title, avg(x3.rating) AS score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2kYaY4N0WGI"
   },
   "outputs": [],
   "source": [
    "query = '''MATCH (p1:User {name: $targetUserName})-[x:RATED]->(movie)<-[x2:RATED]-(p2:User)\n",
    "WHERE p2 <> p1\n",
    "WITH p1, p2, collect(x.rating) AS p1Ratings, collect(x2.rating) AS p2Ratings, \n",
    "    collect(x.timestamp) AS p1Times, collect(x2.timestamp) AS p2Times\n",
    "WHERE size(p1Ratings) > 10\n",
    "WITH p1, p2,\n",
    "       algo.similarity.cosine(p1Ratings, p2Ratings) AS ratingSimilarity,\n",
    "       algo.similarity.cosine(p1Times, p2Times) AS timeSimilarity\n",
    "WITH p1, p2, ((3*ratingSimilarity)+timeSimilarity)/4 AS similarity\n",
    "ORDER BY similarity DESC\n",
    "LIMIT 10\n",
    "\n",
    "MATCH (p2)-[x3:RATED]->(rec:Movie)\n",
    "WHERE NOT EXISTS ( (p1)-[:RATED]->(rec) )\n",
    "RETURN rec.title, avg(x3.rating) AS score\n",
    "ORDER BY score DESC\n",
    "LIMIT 30'''\n",
    "results = session.run(query, parameters={'targetUserName': TARGET_USER_NAME})\n",
    "print(\"RECOMMENDED MOVIES FOR\", TARGET_USER_NAME, \":\")\n",
    "for result in results:\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHKLkSV-0SZz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hdp_wzdhVhph"
   },
   "source": [
    "# Parte 2 - Content-based Filtering\n",
    "\n",
    "Gli algoritmi di tipo *content-based filtering* cercano la similarità tra film utilizzando i loro attributi in comune. La sandbox fornisce esempi che sfruttano genere, attori o registi in comune tra film e utilizzano diverse metriche di similarità per consigliare film simili a quelli che un utente ha maggiormente gradito.\n",
    "\n",
    "Nella prima parte di questa sezione si usa l'algoritmo *tf-idf* per estrarre le parole più importanti della trama di ogni film e si estende il dataset aggiungendo queste parole chiave per ciascun film. Nella seconda parte sono riportate due query che, partendo da alcuni esempi forniti dalla sandbox, mostrano come sfruttare le parole chiave della trama dei film per migliorare la misura di similarità tra essi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Fo6Eg-bcLg6"
   },
   "source": [
    "## Modifica Database\n",
    "In questa seconda parte, la traccia del progetto chiede di:\n",
    "1. Estrarre le parole chiave dai plot dei film;\n",
    "2. Estendere il grafo modellando le parole chiave all'interno di esso;\n",
    "3. Formulare queries che sfruttino queste keyword per fornire *content-based recommendations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMJYUedeJtuc"
   },
   "source": [
    "### Estrazione Keyword\n",
    "Per individuare le parole più importanti all'interno della trama dei film si è scelto di utilizzare una misura chiamata **tf-idf (Term Frequency - Inverse Document Frequency)**. Per ogni parola in ogni documento, vengono enumerate le occorrenze di quel termine all'interno del documento e le occorrenze di quel termine in tutto il corpus di documenti, ovvero tutte le trame dei film. Lo score *tf-idf* è proporzionale al rapporto tra queste due quantità: \n",
    "\n",
    "$$ tf\\_idf(t, d) = tf(t, d) * idf(t) \\\\\n",
    "idf(d, t) = log \\left[ \\frac{(1 + n)}{(1 + df(d, t))} \\right] + 1$$\n",
    "\n",
    "dove $tf(t,d)$ è il numero di volte in cui il termine $t$ compare nel documento $d$, $n$ è il numero di documenti considerati e $df(d,t)$ è il numero di documenti in cui compare il termine $t$. Viene aggiunto 1 al numeratore e al denominatore all'interno del logaritmo per rendere il valore dell'$idf$ più *smooth*, viene aggiunto 1 al logaritmo per evitare di moltiplicare per zero. Più questo score è alto, più una parola appare frequentemente nel documento e meno frequentemente in tutto il corpus, segno della rilevanza di quella parola nella specifica trama.\n",
    "\n",
    "Seguono alcune considerazioni sull'implementazione prima di passare al codice:\n",
    "1. Non sono stati utilizzati tutti i film del dataset per ragioni di tempo computazionale. Considerando solo i film con più di due recensioni si sono considerati circa i 2/3 del dataset. Per un risultato migliore è possibile rimuovere la seconda clausola `WHERE` dalla query nella funzione `get_movies_plot()` in modo da considerare ogni film del dataset, ma così facendo l'esecuzione ci metterà più tempo;\n",
    "2. Per migliorare la qualità del risultato finale viene applicato un preprocessing alla trama di ogni film. Il preprocessing sfrutta le funzionalità offerte dalla libreria  `nltk` ed è composto da:\n",
    "    1. Lowering di tutto il testo;\n",
    "    2. Rimozione della punteggiatura;\n",
    "    3. Tokenizzazione;\n",
    "    4. Rimozione delle stopwords;\n",
    "    5. Stemming.\n",
    "3. Si è scelto di estrarre per ciascun film tre keyword, corrispondenti alle tre parole con TF-IDF maggiore.\n",
    "\n",
    "*Nota: Per parte del codice qui implementato si deve dare credito all'autore di [questo notebook](https://github.com/kavgan/nlp-in-practice/tree/master/tf-idf).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am1hFcw3eBXn"
   },
   "outputs": [],
   "source": [
    "'''Query and preprocess movie plots'''\n",
    "def get_movies_plots():\n",
    "    query = '''MATCH (m:Movie)<-[r:RATED]-(:User)\n",
    "        WHERE EXISTS(m.plot)\n",
    "        WITH m, count(r) AS n_revs\n",
    "        WHERE n_revs > 2\n",
    "        RETURN m.title, m.plot\n",
    "    '''\n",
    "    results = session.run(query)\n",
    "    data = dict()\n",
    "    for result in results:\n",
    "        data[result['m.title']] = result['m.plot']\n",
    "    \n",
    "    filtered_data = dict()\n",
    "    for movie in data.keys():\n",
    "        # Lowering\n",
    "        plot = data[movie].lower()\n",
    "        # Remove punctuation\n",
    "        plot = plot.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        # Tokenize\n",
    "        word_tokens = word_tokenize(plot)\n",
    "        # Filter stop words\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        # Stem\n",
    "        ps = PorterStemmer()\n",
    "        newlist = []\n",
    "        for word in filtered_sentence:\n",
    "            newlist.append(ps.stem(word))\n",
    "        filtered_data[movie] = ' '.join(newlist)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "'''Sort a matrix in COOrdinate format'''\n",
    "def sort_coo(coo_matrix):\n",
    "    # Transform matrix in a list of tuple with shape [(keyword_index, tf-idf_score), ...]\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    # Sort for descending tf-idf score\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "\n",
    "'''Get the keyword names and TF-IDF score of top n items'''\n",
    "def extract_topn_from_vector(keyword_names, sorted_items, topn=3):\n",
    "    # Use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    results = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        # Get keyword name from keyword index\n",
    "        fname = keyword_names[idx]\n",
    "        results.append(fname)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbCIhw3Un3Cu"
   },
   "outputs": [],
   "source": [
    "dataset = get_movies_plots()\n",
    "docs = list(dataset.values())\n",
    "# Discard all the words that appear in more than the 70% of the movies.\n",
    "cv=CountVectorizer(max_df=0.70, max_features=2000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "# Compute idf values\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "keyword_names=cv.get_feature_names()\n",
    "movie_keywords = {}\n",
    "keywords_set = {}\n",
    "\n",
    "for movie in dataset.keys():\n",
    "    # Compute tf-idf score for each word and extract the 3 words with\n",
    "    # highest score\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([dataset[movie]]))\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(keyword_names,sorted_items)\n",
    "    for key in keywords:\n",
    "        if key in keywords_set:\n",
    "            keywords_set[key] += 1\n",
    "        else:\n",
    "            keywords_set[key] = 1\n",
    "    movie_keywords[movie] = keywords\n",
    "\n",
    "# Print top keywords\n",
    "print ({k: v for k, v in sorted(keywords_set.items(), key=lambda item: item[1], reverse=True) if v > 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTgZlkXRdJ54"
   },
   "source": [
    "### Modifica grafo\n",
    "Una volta estratte le informazioni necessarie dal dataset, è necessario inserirle nel grafo per poterle sfruttare nelle nostre queries. Ci sono due alternative per espandere il grafo con queste informazioni:\n",
    "1. Aggiungere ad ogni nodo `:Movie` un attributo di tipo lista contenente le tre keyword estratte per esso;\n",
    "2. **Aggiungere le parole chiave come nodi del grafo con label `:Tag` e creare una relazione `:HAS_TAG` che leghi ogni film alle keyword relative ad esso.**\n",
    "\n",
    "Si è scelto di utilizzare la seconda soluzione poichè rende possibile sfruttare l'espressività e la flessibilità dei *path* tra i nodi, oltre che a semplificare il codice delle queries che seguiranno per identificare *tags* comuni tra i nodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5EX1yahdL3O"
   },
   "outputs": [],
   "source": [
    "'''Add tags and relationships to a Neo4j dataset\n",
    "\n",
    "Given a set oh tags a map between movies and related tags, create tag nodes and\n",
    "create relationships between movies and tags.\n",
    "'''\n",
    "def add_tags_database(map, tags):\n",
    "    create_tag_query = 'CREATE (:Tag {name: $tag})'\n",
    "    for tag in tags:\n",
    "        session.run(create_tag_query, parameters={'tag': tag})\n",
    "    \n",
    "    create_tag_relationship_query = '''MATCH (m:Movie {title: $movie}), (t:Tag {name: $tag})\n",
    "    CREATE (m)-[:HAS_TAG]->(t)'''\n",
    "    for movie, tags in map.items():\n",
    "        for tag in tags:\n",
    "            session.run(create_tag_relationship_query, \n",
    "                        parameters={'movie': movie, 'tag': tag})\n",
    "\n",
    "\n",
    "add_tags_database(movie_keywords, keywords_set.keys())\n",
    "\n",
    "test_db_query = '''MATCH (m:Movie)-[r:HAS_TAG]->(t:Tag)\n",
    "RETURN t.name, count(r) AS movies\n",
    "ORDER BY movies DESC\n",
    "LIMIT 50'''\n",
    "results = session.run(test_db_query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TFgmoI-3jlA"
   },
   "source": [
    "## Query 2.1 - Weighted Content Algorithm\n",
    "\n",
    "Questa query è un'estensione dell'esempio che sfrutta il *weighted content algorithm* per misurare la similarità tra due film. Questo algoritmo considera il numero di generi, attori e registi in comune tra due film, assegna un peso a ciascuno di questi elementi in base a quanto essi siano significativi per la somiglianza tra i film (ad esempio, due film dello stesso genere è probabile che siano più simili di due film che hanno lo stesso regista ma generi diversi) e calcola uno score di somiglianza come una somma pesata degli elementi in comune.\n",
    "\n",
    "La query seguente estende questo esempio considerando anche i tag in comune: visto il ridotto numero di tag, è raro che due film condividano uno stesso tag, quindi è ragionevole associare un peso alto al numero di tag in comune. Inoltre, un tag in comune è significativo di due trame simili tra loro, ciò rende i tag un elemento significativo di similarità: per questo si è deciso di associare ad essi lo stesso peso dei generi, il più alto tra quelli associati ai diversi elementi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZwIJ4-u3gKz"
   },
   "outputs": [],
   "source": [
    "wca_query = '''MATCH (m:Movie) WHERE m.title = \"Inception\"\n",
    "MATCH (m)-[:IN_GENRE]->(g:Genre)<-[:IN_GENRE]-(rec:Movie)\n",
    "\n",
    "WITH m, rec, COUNT(*) AS gs\n",
    "\n",
    "OPTIONAL MATCH (m)<-[:ACTED_IN]-(a:Actor)-[:ACTED_IN]->(rec)\n",
    "WITH m, rec, gs, COUNT(a) AS as\n",
    "\n",
    "OPTIONAL MATCH (m)<-[:DIRECTED]-(d:Director)-[:DIRECTED]->(rec)\n",
    "WITH m, rec, gs, as, COUNT(d) AS ds\n",
    "\n",
    "OPTIONAL MATCH (m)-[:HAS_TAG]->(t:Tag)<-[:HAS_TAG]-(rec)\n",
    "WITH m, rec, gs, as, ds, COUNT(t) as ts\n",
    "\n",
    "RETURN rec.title AS recommendation, (5*ts)+(5*gs)+(3*as)+(4*ds) AS score ORDER BY score DESC LIMIT 100'''\n",
    "\n",
    "results = session.run(wca_query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_69IuWAb6Ei"
   },
   "source": [
    "## Query 2.2 - Jaccard similarity\n",
    "\n",
    "la *Jaccard similarity* è una vera e propria misura di similarità. Dati due insiemi di elementi $A$ e $B$, essa si definisce come il rapporto tra il numero di elementi nell'intersezione dei due insiemi e il numero di elementi nell'unione dei due insiemi:\n",
    "\n",
    "$$ J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
    "\n",
    "Due insiemi identici hanno una similarità di 1, due insiemi senza elementi in comune hanno similarità 0. La sandbox fornisce un esempio in cui gli insiemi $A$ e $B$ sono dati dall'insieme di attori, generi e registi di due film: in questo modo possiamo utilizzare la *Jaccard Similarity* per calcolare la similarità tra due film in termini di caratteristiche condivise tra i due.\n",
    "\n",
    "La query seguente estende l'esempio della sandbox considerando anche i tag nell'insieme degli elementi di un film. Per comprendere più facilmente il codice *cypher*, si spiega il significato degli *alias* utilizzati nella query:\n",
    "\n",
    "* `intersection`: numero di elementi in comune tra due film;\n",
    "* `s1`: elementi presenti nel set del film di riferimento;\n",
    "* `s2`: elementi presenti nel set del film che vogliamo confrontare;\n",
    "* `union`: insieme dato dall'unione di `s1` e `s2`;\n",
    "* `jaccard`: $\\frac{intersection}{|union|}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgzlKzN_b_1C"
   },
   "outputs": [],
   "source": [
    "jaccard_similarity_query = '''\n",
    "MATCH (m:Movie {title: \"Inception\"})-[:IN_GENRE|:ACTED_IN|:DIRECTED|:HAS_TAG]-(t)<-[:IN_GENRE|:ACTED_IN|:DIRECTED|:HAS_TAG]-(other:Movie)\n",
    "WITH m, other, COUNT(t) AS intersection, COLLECT(t.name) AS i\n",
    "\n",
    "MATCH (m)-[:IN_GENRE|:ACTED_IN|:DIRECTED|:HAS_TAG]-(mt)\n",
    "WITH m,other, intersection,i, COLLECT(mt.name) AS s1\n",
    "\n",
    "MATCH (other)-[:IN_GENRE|:ACTED_IN|:DIRECTED|:HAS_TAG]-(ot)\n",
    "WITH m,other,intersection,i, s1, COLLECT(ot.name) AS s2\n",
    "\n",
    "WITH m,other,intersection,s1,s2\n",
    "\n",
    "WITH m,other,intersection,s1+filter(x IN s2 WHERE NOT x IN s1) AS union, s1, s2\n",
    "\n",
    "RETURN m.title, other.title,((1.0*intersection)/SIZE(union)) AS jaccard ORDER BY jaccard DESC LIMIT 100'''\n",
    "\n",
    "results = session.run(jaccard_similarity_query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vn9oYjeiOw8P"
   },
   "source": [
    "# Extras\n",
    "Per riportare il database Neo4j allo stato precedente all'aggiunta dei tag, rimuovere i commenti dalla *code cell* seguente ed eseguirla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P46xLfUuOztf"
   },
   "outputs": [],
   "source": [
    "# delete_tags_query = '''MATCH (t:Tag)\n",
    "# DETACH DELETE (t)'''\n",
    "# session.run(delete_tags_query)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_BDN_Attivita.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
